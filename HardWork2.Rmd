---
title: "Hard Work 2"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
---


## Instructions

1. Study Sections 2.1-2.10 of Chapter 2: "Inferences in Regression and Correlation Analysis."    
<span id="note">(Pace yourself, you have two weeks to work on this assignment.)</span>

2. Attempt and submit at least <span id=points style="padding-left:0px;">{83}</span> Hard Work Points by Saturday at 11:59 PM.    
<span id=note>Over <span id=points style="padding-left:0px;">{92}</span> gets you {+1} Final Exam Point.</span>    
<span id=note>Over <span id=points style="padding-left:0px;">{100}</span> gets you {+2} Final Exam Points.</span>    
<span id=note>Over <span id=points style="padding-left:0px;">{107}</span> gets you {+3} Final Exam Points.</span>

<br/>

##Notes

There is always a hypotheses for each parameter in the model. $beta_0$ and $beta_1$ are usually equal to or not equal to zero.

$b_0$ - $beta_{00}$

This is the PT function

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(mosaic)
library(ggplot2)
library(dplyr)
library(tidyverse)

plot(speed~dist, data = cars)
cars.lm <- lm(speed ~ dist, data = cars)
summary(cars.lm)

#this is the way we calculate our p-value by hand
#Pr(>\t\) just means that the probablility is two-tailed 

2*pt(9.474, 48, lower.tail = FALSE)

#this is calculating the critical value. 0.25 is our alpha or level of significance
qt(1-0.25/2, 48)

#This is the confidence interval equation and margin of error
#  b0 +or- t(1-a/2; n-2)S{b0}

8.28391 + qt(1-0.05/2, 48)*0.87438
8.28391 - qt(1-0.05/2, 48)*0.87438

confint(cars.lm)

#Also confint(cars.lm, level = (confidence interval))
#this is how to calculate the residuals

sum(cars.lm$residuals^2)

sqrt(sum(cars.lm$residuals^2)/48)

#prediction interval
predict(cars.lm, data.frame(dist=50), interval = "prediction")

predict(cars.lm, data.frame(dist=50), interval = "confidence")


```

## Reading Points <span id=headpoints>{40} Possible</span>

### Section 2.1 <span id=recpoints>{6}</span><span id=report>{ 5/6 }</span>

### Section 2.2 <span id=recpoints>{3}</span><span id=report>{ 3/3 }</span>

### Section 2.3 <span id=recpoints>{1}</span><span id=report>{ 1/1 }</span>

### Section 2.4 <span id=recpoints>{4}</span><span id=report>{ 3/4}</span>

### Section 2.5 <span id=recpoints>{6}</span><span id=report>{ 5/ 6}</span>

### Section 2.6 <span id=recpoints>{3}</span><span id=report>{ 3/3}</span>

### Section 2.7 <span id=recpoints>{8}</span><span id=report>{ 6/ 8}</span>

### Section 2.8 <span id=recpoints>{4}</span><span id=report>{ 3/ 4}</span>

### Section 2.9 <span id=recpoints>{3}</span><span id=report>{ 3/3 }</span>

### Section 2.10 <span id=recpoints>{2}</span><span id=report>{ 2/2 }</span>


<br/>




## Theory Points <span id=headpoints>{33} Possible</span>

### Problem 2.1 <span id=points>{1}</span><span id=report>{ / }</span>
 
### Problem 2.2 <span id=points>{1}</span><span id=report>{ / }</span>
 
### Problem 2.3 <span id=points>{1}</span><span id=report>{ / }</span>

### Problem 2.9 <span id=points>{1}</span><span id=report>{ / }</span>
 
### Problem 2.11 <span id=points>{1}</span><span id=report>{ / }</span>
 
### Problem 2.12 <span id=points>{1}</span><span id=report>{ / }</span>

### Problem 2.18 <span id=points>{1}</span><span id=report>{ / }</span>
<span id=headnote>Hint: Read page 71</span>
 
### Problem 2.19 <span id=points>{1}</span><span id=report>{ / }</span>

### Problem 2.20 or 2.21 or 2.22 <span id=points>{1}</span><span id=report>{ / }</span>

### Problem 2.33 <span id=recpoints>{3}</span><span id=report>{ 3/3 }</span>

a.
$$
H_0 = 0
$$
$$
H_a neq 0
$$

b. 

Full Model:
$$
y_i = \beta_0 + \beta_1x_i + \epsilon_i
$$
Reduced model:
$$
y_i = \beta_0 +\epsilon_i
$$

c.
We cannot determine dfr - dff because we don't have a sample size for the data. 
### Problem 2.50 <span id=points>{2}</span><span id=report>{ / }</span>

### Problem 2.51 <span id=points>{2}</span><span id=report>{ / }</span>

### Problem 2.53 <span id=points>{4}</span><span id=report>{ / }</span>

### Problem 2.66 <span id=recpoints>{5}</span><span id=report>{ 3/5 }</span> 

a. 

```{r}

x <- c(4,8,12,16,20)

e <- rnorm(5, 0, sqrt(25))


y <- 20 + 4 * x + e

p2.66.lm <- lm(y~x)

b <- p2.66.lm$coefficients

print(b)



predict(p2.66.lm, data.frame(x=10), interval = "confidence")
```

``{r}
for (i in vector)
``
b. 

<span id=note>R-Code Hint: rnorm(5, 0, sqrt(25))</span>

### Exercise 2.55 <span id=recpoints>{2}</span><span id=report>{ / }</span>

### Exercise 2.57 <span id=points>{3}</span><span id=report>{ / }</span>

### Exercise 2.61 <span id=points>{3}</span><span id=report>{ / }</span>

 

<br/>



## Application Points <span id=headpoints>{53} Possible</span>

<a id=datalink target="_blank" href=http://www.stat.ufl.edu/~rrandles/sta4210/Rclassnotes/data/textdatasets/index.html>Data Files</a>


### Problem 2.1 <span id=points>{1}</span><span id=report>{ / }</span>

### Problem 2.4 <span id=recpoints>{4}</span><span id=report>{ 4/4 }</span>

```{r p4}
# Load the Data:
p1.19 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR19.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.19) <- c("Y","X")

p1.19.lm <- lm(p1.19)

summary(p1.19.lm)

confint(p1.19.lm, level = 0.99)

print(confint(p1.19.lm, level = 0.99))

qt(1-0.01/2, length(p1.19$X-2))
```

a. our 99% confidence interval for $\beta_0$ is:

0.00538 < Y < 0.72268

b. 

$H_0: \beta_1 = 0$
$H_a: \beta_1 \ne 0$

If |t*| <= t(1-$\alpha$/2; n-2), conclude $H_o$
If |t*| >= t(1-$\alpha$/2; n-2), conclude $H_a$

t* = 3.040

t = 2.618137

conclude $H_o$


c. 

Our P-value is : 0.00292

### Problem 2.5 <span id=recpoints>{4}</span><span id=report>{ 4/4 }</span> 

```{r p5}
# Load the Data:
p1.20 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR20.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.20) <- c("Y","X")

p1.20.lm <- lm(p1.20)

summary(p1.20.lm)

qt(1-0.90/2, length(p1.20$X) - 2)
```

a. It is estimated that there is an increased 15.04 for every printer that is serviced.

b. 
$H_0: \beta_1 = 0$
$H_a: \beta_1 \ne 0$

If |t*| <= t(1-$\alpha$/2; n-2), conclude $H_o$
If |t*| >= t(1-$\alpha$/2; n-2), conclude $H_a$

t* = 31/123

t = 0.1264

conclude $H_a$

c. Yes, because we conclude that there is an association between the variables, we can assume that for every printer serviced, the time increases. 

d.
We see that $\beta1$ is equal to 15.0352. This means that they are one minute over. The P-value is low, so we can assume that this is appropriate. 

e. $\beta0$ doesn't give any relevant information because we would have negative time used if that was the case and that's impossible. 





### Problem 2.7 <span id=points>{3}</span><span id=report>{ / }</span> 

```{r p7}
# Load the Data:
p1.22 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR22.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.22) <- c("Y","X")
```


### Problem 2.10 <span id=recpoints>{2}</span><span id=report>{ 2/2 }</span> 

a. It is appropriate because there are multiple times when the temperature could be set to 31 degrees  

b. It depends on the size of the population. If we're are sampling the entire U.S. population, I would say yes because there could be several families that would have the same income, but if it was just a city or region, I would say no. 

c.Yes, because the x value will remain the same and we can have a confidence interval when there are multiple yi values for the same x value. 



### Problem 2.13 <span id=recpoints>{4}</span><span id=report>{ 3/4 }</span> 

```{r p13}
# Load the Data:
p1.19 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR19.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.19) <- c("Y","X")

summary(p1.19.lm)

```

yi = 2.11405 + 0.03883 *28  

ACT = 28
GPA = 3.20129

```{r}
confint(p1.19.lm)

predict(p1.19.lm, data.frame(X=28), interval = "confidence")

predict(p1.19.lm, data.frame(X=28), interval = "prediction")

```

a.
This means that we are 95% confident that the mean for a Freshman GPA who's ACT score is 28 is between 3.0613 and 3.410

b. 
This means that we are 95% confident that her GPA will be between 1.959 and 4.443063. This doesn't mean a lot because it's impossible to get above a 4.0.

c.
The prediction interval is greater than the confidence interval.

d.

``{r}

# Create a confidence bands function:
confbands <- function(lmObject, xh=NULL, alpha=0.05){
  
  # Use some fancy code to get the data out of the lmObject
  # while knowing which variable was x and which was y.
  thecall <- strsplit(as.character(lmObject$call[2]), "~")
  yname <- gsub(" ", "", thecall[[1]][1])
  xname <- gsub(" ", "", thecall[[1]][2])
  theData <- lmObject$model
  theData <- theData[,c(yname,xname)]
  colnames(theData) <- c("Y","X")

  # Begin creating confidence bands  
  n <- nrow(theData)
  W2 <- 2*qf(1-alpha, 2, n-2)
  SSE <- sum( lmObject$res^2 )
  MSE <- SSE/(n-2)
  s2.Yhat.h <- function(xh){
    MSE*(1/n + (xh - mean(theData$X))^2/sum( (theData$X - mean(theData$X))^2 ))
  }
  b <- coef(lmObject)
  
  # Add upper bound to scatterplot
  curve(b[1]+b[2]*x + W2*sqrt(s2.Yhat.h(x)), add=TRUE)

  # Add lower bound to scatterplot
  curve((b[1]+b[2]*x) - W2*sqrt(s2.Yhat.h(x)), add=TRUE)
  
  if (!is.null(xh)){
    tmp <- c(b[1]+b[2]*xh - W2*sqrt(s2.Yhat.h(xh)), b[1]+b[2]*xh + W2*s2.Yhat.h(xh))
    names(tmp) <- c("Lower","Upper")
    tmp
  }
}


plot(Y~Y, data = p1.19)
abline(p1.19.lm)
confbands(p1.19.lm, xh=28)
``




### Problem 2.14 <span id=points>{4}</span><span id=report>{ / }</span> 

```{r p14}
# Load the Data:
p1.20 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR20.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.20) <- c("Y","X")
```


### Problem 2.15 <span id=points>{4}</span><span id=report>{ / }</span>

```{r p15}
# Load the Data:
p1.21 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR21.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.21) <- c("Y","X")
```


### Problem 2.16 <span id=recpoints>{5}</span><span id=report>{ 3/5 }</span> 

```{r p16}
# Load the Data:
p1.22 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR22.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.22) <- c("Y","X")

p1.22.lm <- lm(p1.22)

predict(p1.22.lm, data.frame(X=30), level = 0.98, interval = "confidence")

predict(p1.22.lm, data.frame(X=30), level = 0.98, interval = "prediction")

predict(p1.22.lm, data.frame(X=30), level = 0.98, interval = "prediction")



```

a. The mean hardness for plastics that are left in for 30 hours is between 227.4569 and 231.8056 at a confidence interval of 0.98. 

b. The prediction interval for hardness for a piece left in for 30 hours is between 220.8695 and 238.393.

c.

d. It should be smaller because when we have a number of samples than according to the law of large numbers, we get closer to the truth. 
e. 


### Problem 2.25 <span id=recpoints>{5}</span><span id=report>{ 5/5 }</span>

```{r p25}
# Load the Data:
p1.20 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR20.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.20) <- c("Y","X")

p1.21.lm <- lm(p1.21)

summary(p1.21.lm)

qf(1-0.05, 1, length(p1.21$X)-2)

anova(p1.21.lm)

```

a. 


b. 
$H_0: \beta_1 = 0$
$H_a: \beta_1 \ne 0$

F* = 72.727

If F* <= F((1-$\alpha$; 1, n -2)) then conclude $H_0$
If F* >= F((1-$\alpha$; 1, n -2)) then conclude $H_a$ 

We conclude the alternative hypothesis.

c. What is t* for the test in b what 

```{r}
sqrt(72.727)
```


8.528

d. Calculate R2 and r. What proportion of the variation in Y is accounted for by introducing X into the regression model?

```{r}

SSTO <- 160 + 17.6

160 / SSTO

sqrt(160/SSTO)

```

R^2 = 0.9009
r = 0.9491

### Problem 2.26 <span id=recpoints>{4}</span><span id=report>{ 4/4 }</span>

```{r p26}
# Load the Data:
p1.22 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR22.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.22) <- c("Y","X")


p1.22.lm <- lm(Y~X, data = p1.22)


anova(p1.22.lm)

ssto <- 5297.5 + 146.4

5297.5 / SSTO

sqrt(160/SSTO)

qf(1-0.01, 1, length(p1.22$X) - 2)


```
a. done
b. 
$H_0: \beta_1 = 0$
$H_a: \beta_1 \ne 0$

F* = 506.51

If F* <= F((1-$\alpha$; 1, n -2)) then conclude $H_0$
If F* >= F((1-$\alpha$; 1, n -2)) then conclude $H_a$ 
F((1-$\alpha$; 1, n -2)) = 8.86159


We conclude $H_a$

c. 

```{r}
sse <- p1.22$Y + p1.22.lm$residuals
ssr <- p1.22$Y + (p1.22.lm$residuals - mean(p1.22$Y))

ggplot(p1.22, aes(x = X, y = Y)) +
  geom_point() +
  geom_point(aes(x = X, y = sse, color = "red")) +
  geom_point(aes(x = X, y = ssr, color = "blue"))+
  stat_smooth(method = "lm", col = "red")

r2 <- sum(ssr)^2 / sum(p1.22$Y - mean(p1.22$Y))^2 

sqrt( 0.9712)

```

I know that the points don't line up exactly, but it seems that SSR has a smaller spread than the SSE. Therefore, I would assume that SSE is a greater component to SSTO. This means that R^2 is closer to zero. 

d. 

R2 = 0.9712
r = 0.98549



### Problem 2.29 <span id=recpoints>{5}</span><span id=report>{ 4/ 5}</span> 

```{r p29}
# Load the Data:
p1.27 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR27.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.27) <- c("Y","X")

p1.27.lm <- lm(p1.27)


ei <- p1.27$Y + p1.27.lm$residuals

ssr <- 

ggplot(p1.27, aes(x = X, y = Y)) +
  geom_point() +
  geom_point(aes(x = X, y = ei, color = "red")) +
  stat_smooth(method = "lm", col = "red")

p1.27.aov <- anova(p1.21.lm)

p1.27.aov

SSTO <- p1.27.aov$`Sum Sq`[1] + p1.27.aov$`Sum Sq`[2]

R2 <- p1.27.aov$`Sum Sq`[1] / SSTO

qf(1 - 0.05,1,length(p1.27$X))



```

c

$H_0: \beta_1 = 0$
$H_a: \beta_1 \ne 0$

F* = 72.727

If F* <= F((1-$\alpha$; 1, n -2)) then conclude $H_0$
If F* >= F((1-$\alpha$; 1, n -2)) then conclude $H_a$ 

F((1-$\alpha$; 1, n -2)) = 4.001191

We conclude $H_a$


d. Not sure what this is asking

e. 
R2 = 0.9009009
r = 0.949158

### Problem 2.30 <span id=points>{5}</span><span id=report>{ / }</span> 

```{r p30}
# Load the Data:
p1.28 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR28.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.28) <- c("Y","X")
```

### Problem 2.32 <span id=points>{3}</span><span id=report>{ / }</span> 

```{r p32}
# Load the Data:
p1.28 <- read.table("https://netfiles.umn.edu/users/nacht001/www/nachtsheim/Kutner/Chapter%20%201%20Data%20Sets/CH01PR28.txt")

# Rewrite the column names to be "Y" and "X" to match the textbook:
colnames(p1.28) <- c("Y","X")
```



<br />



 

 
 
 
 <style>
#points {
  font-size:.8em;
  padding-left:5px;
  font-weight:bold; 
  color:#317eac;
}

#recpoints {
  font-size:.8em;
  padding-left:5px;
  font-weight:bold; 
  color:#7eac31;
}

#report {
  font-size:.7em;
  padding-left:15px;
  font-weight:normal; 
  color:#5a5a5a;
}

#datalink {
  font-size:.5em;
  color:#317eac;
  padding-left:5px;
}

#headnote {
  font-size:.6em;
  color:#787878;
}

#note {
  font-size:.8em;
  color:#787878;
}

#headpoints {
 font-size:12pt;
 color: #585858; 
 padding-left: 15px;
}
</style>


 

 